{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 1. Basic Text Pre-processing\n",
    "#### Student Name: Wing Hang CHAN\n",
    "#### Student ID: 3939713\n",
    "\n",
    "Date: 15-Sep-2022\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "There are 776 job advertistment in 4 different folders. A class ```JobAd``` for storing infomation like file path, raw data, job category, content dictionary, etc. The class provide functions for loading raw data, tokenizing description, counting tokens, etc. The class can be re-used in Task 2 & 3. Below notebook follows the assignment requirement and saves a file vocab.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "\n",
    "There are many external libraries imported in internal modules\n",
    "1. from itertools import chain\n",
    "1. from nltk.tokenize import RegexpTokenizer\n",
    "1. from nltk.tokenize import sent_tokenize\n",
    "1. import re\n",
    "1. import os\n",
    "1. from itertools import chain\n",
    "1. import numpy as np\n",
    "1. import pandas as pd\n",
    "1. from nltk.probability import *\n",
    "1. from scipy.sparse import csr_matrix\n",
    "1. from sklearn.linear_model import LogisticRegression\n",
    "1. from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal module\n",
    "from module.jobAd import JobAd\n",
    "from module.Utils import *\n",
    "\n",
    "# external libraries\n",
    "from itertools import chain\n",
    "from nltk.probability import *\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Examining and loading data\n",
    "- Examine the data folder, including the categories and job advertisment txt documents, etc. Explain your findings here, e.g., number of folders and format of txt files, etc.\n",
    "- Load the data into proper data structures and get it ready for processing.\n",
    "- Extract webIndex and description into proper data structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function ```read_job_ad``` is to\n",
    " 1. read all files from different sub-folders (i.e. job categories)\n",
    " 2. convert all files in ```JobAd``` object for pre-processing\n",
    " 3. read sub-folder name as job category\n",
    " 4. ```JobAd``` stores all contents from a file (e.g. title, webIndex, company, description, job category)\n",
    " 5. return a list of all ```JobAd``` objects and a list of string with all job categories"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folders: 4, ['Accounting_Finance', 'Engineering', 'Healthcare_Nursing', 'Sales']\n",
      "Number of files: 776\n",
      "{\"dict_keys(['Title', 'Webindex', 'Company', 'Description'])\", \"dict_keys(['Title', 'Webindex', 'Description'])\"}\n"
     ]
    }
   ],
   "source": [
    "result = read_job_ad(\"./data\")\n",
    "job_ad_list = result[0]\n",
    "job_category = result[1]\n",
    "\n",
    "print(\"Number of folders: {}, {}\".format(len(job_category), job_category))\n",
    "print(\"Number of files: {}\".format(len(job_ad_list)))\n",
    "print(set([str(job_ad.data.keys()) for job_ad in job_ad_list]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the above output, there are 4 job categories. They are 'Accounting_Finance', 'Engineering', 'Healthcare_Nursing' & 'Sales'.\n",
    "There are 776 job advertisements. From the output, there are some job advertisements having 4 types of content, i.e. 'Title', 'Webindex', 'Company' & 'Description'. But, there are some job advertisements do not have 'Company'."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pre-processing data\n",
    "Perform the required text pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tokenizing Description (Pt. 1, 2 & 3)\n",
    "1. Converting words into lower case\n",
    "2. Tokenize job advertisement description with regular expression,\n",
    "    ```python\n",
    "    r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "    ```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 186952\n",
      "Vocabs: 9834\n"
     ]
    }
   ],
   "source": [
    "# start tokenizing job advertisement description\n",
    "for job_ad in job_ad_list:\n",
    "    job_ad.tokenizeDesc(pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\") # Task 1 pt. 1, 2 & 3\n",
    "\n",
    "all_words = summarise_words(job_ad_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are total 186952 words and 9834 vocabs (i.e. distinct of words) tokenized with the above regular expression for all descriptions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Remove words with length less than 2 (Pt. 4)\n",
    "```JobAd``` function ```remove_words(length=2)``` is to keep description tokens with character is larger than or equal to ```length```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 180913\n",
      "Vocabs: 9808\n",
      "Removed Words: 6039\n",
      "Removed Vocabs: 26\n"
     ]
    }
   ],
   "source": [
    "for job_ad in job_ad_list:\n",
    "    job_ad.remove_words() # Task 1 pt. 4\n",
    "\n",
    "all_words_removed = summarise_words(job_ad_list, all_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are 6039 words and 26 vocabs with length is 1.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Remove stopwords (Pt.5)\n",
    "1. ```Utils``` function ```read_stopwords(path=\"stopwords_en.txt\", permission=\"r\", encoding=\"utf-8\")``` is to read stopwords from files\n",
    "2. ```JobAd``` function ```remove_by_list(self, remove_list)``` is to remove description tokens with provided list of words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "stopword_lists = read_stopwords()\n",
    "\n",
    "for job_ad in job_ad_list:\n",
    "    job_ad.remove_by_list(stopword_lists) #Task 1 pt. 5\n",
    "\n",
    "all_words_removed_stopword = summarise_words(job_ad_list, all_words_removed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 107161\n",
      "Vocabs: 9404\n",
      "Removed Words: 73752\n",
      "Removed Vocabs: 404\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are 73752 words and 404 vocabs treated as stopwords."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Remove word that appears only once based on Term Frequency (Pt.6)\n",
    "to remove all words from all job advertisements which appear 1 only\n",
    "1. by using ```nltk.FreqDist``` for counting frequency\n",
    "2. ```[job_ad.desc_tokens for job_ad in job_ad_list]``` => join all desc_tokens from all job advisements (i.e. [all job_ad.desc_tokens[tokens]])\n",
    "3. ```chain.from_iterable([all job_ad.desc_tokens[tokens]])``` => join [all tokens]\n",
    "4. ```FreqDist[0]``` is the word\n",
    "5. ```FreqDist[1]``` is the number of word frequency within the list provided\n",
    "6. use ```JobAd``` function ```remove_by_list``` to remove the words in description tokens\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 102975\n",
      "Vocabs: 5218\n",
      "Removed Words: 4186\n",
      "Removed Vocabs: 4186\n"
     ]
    }
   ],
   "source": [
    "words = list(chain.from_iterable([job_ad.desc_tokens for job_ad in job_ad_list]))\n",
    "term_fd = FreqDist(words)\n",
    "\n",
    "# Filtering a word list which appear once only\n",
    "appear_1_list = [fd[0] for fd in list(term_fd.items()) if fd[1] == 1]\n",
    "\n",
    "# remove word appear once\n",
    "for job_ad in job_ad_list:\n",
    "    job_ad.remove_by_list(appear_1_list)\n",
    "\n",
    "all_words_removed_appear_1 = summarise_words(job_ad_list, all_words_removed_stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are 4186 words and vocabs appears only once combining all documents together."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Remove top 50 most frequent words base on Document Frequency (Pt.7)\n",
    "to remove the top 50 most appearance words for all job advertisement by counting 1 for appearance from each job advertisement\n",
    "1. ```[set(job_ad.desc_tokens) for job_ad in job_ad_list]``` => join all desc_tokens\n",
    "2. with distinct for all job advisement i.e. ```[distinct job_ad.desc_tokens[tokens]]```\n",
    "3. ```chain.from_iterable([all job_ad.desc_tokens[tokens]])``` => join [all tokens]\n",
    "4. Use ```FreqDist``` function ```most_common()``` to get the list of top 50 document frequency\n",
    "5. use ```JobAd``` function ```remove_by_list()``` to remove description tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 81205\n",
      "Vocabs: 5168\n",
      "Removed Words: 21770\n",
      "Removed Vocabs: 50\n"
     ]
    }
   ],
   "source": [
    "words_2 = list(chain.from_iterable([set(job_ad.desc_tokens) for job_ad in job_ad_list]))\n",
    "doc_fd = FreqDist(words_2)  # compute document frequency for each unique word/type\n",
    "\n",
    "# Filtering a word list which includes 50 most common \n",
    "most_50_common_keys = [fd[0] for fd in doc_fd.most_common(50)]\n",
    "\n",
    "for job_ad in job_ad_list:\n",
    "    job_ad.remove_by_list(most_50_common_keys)\n",
    "\n",
    "final_words_list = summarise_words(job_ad_list, all_words_removed_appear_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The top 50 most frequent vocabs which include in all description for 21770 times.\n",
    "There are 81205 words and 5168 vocabs left for description tokens."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving required outputs (Pt. 8 & 9)\n",
    "Save the vocabulary, bigrams and job advertisement txt as per specification.\n",
    "- vocab.txt (Pt.9)\n",
    "- job_ad_all.txt (for Task 2, also requirement for Task 1 Pt.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save output data...\n",
    "words = list(chain.from_iterable([job_ad.desc_tokens for job_ad in job_ad_list]))\n",
    "fd = FreqDist(words)\n",
    "\n",
    "# save files with vocab.txt, w+ for saving file with not exist file\n",
    "with open(\"vocab.txt\",\"w+\",encoding= 'utf-8') as f: # open the txt file\n",
    "    # sort the list by words\n",
    "    for idx, k in enumerate(sort(list(fd.keys()))):\n",
    "        # write in file by lines word:index\n",
    "        f.write(\"{}:{}\\n\".format(k, idx))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "generate_corpus_file(job_ad_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Summary\n",
    "Pre-processing should include stemming and lemmatization. There are some words could be combined(e.g. communicate, communicated, communicating, communication & communicator).<br/>\n",
    "There are some job advertisement description which is not a complete sentence. The regex pattern ```r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"``` may filter some web url.<br/>\n",
    "Also, there are some short-form terms like ASAP will be changed to asap which is not easy to recognized as a short-form term in lower case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "advprog-venv",
   "language": "python",
   "display_name": "advProg-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}